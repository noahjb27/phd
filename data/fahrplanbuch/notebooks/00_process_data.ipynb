{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_process_data.ipynb\n",
    "\n",
    "\"\"\"\n",
    "This notebook processes raw Berlin transport data from Fahrplanbücher into structured formats.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Import processing modules\n",
    "from utils.data_loader import DataLoader, format_line_list\n",
    "from processor import TransportDataProcessor\n",
    "# --- Import the correct matcher ---\n",
    "from df_station_matcher import DataFrameStationMatcher\n",
    "# from db_station_matcher import Neo4jStationMatcher # <-- Can likely remove this import\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Transport Data Processing\n",
    "\n",
    "This notebook performs the initial extraction and transformation of Berlin's historical public transportation data from raw sources. It represents the first step in our processing pipeline.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "1. **Data Extraction**: Load and parse raw data from digitized Fahrplanbücher (timetables)\n",
    "2. **Initial Structuring**: Convert raw data into structured tables with consistent formats\n",
    "3. **Station Identification**: Establish unique identifiers for transportation stops\n",
    "4. **Preliminary Geolocation**: Match stations to known geographic coordinates where possible\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process follows these key steps:\n",
    "1. Load raw data from CSV files containing transcribed Fahrplanbuch information\n",
    "2. Process this data into standardized tables (lines and stops)\n",
    "3. Match stations with existing station records to obtain geographic coordinates\n",
    "4. Generate interim data files for subsequent processing stages\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "The data represents Berlin's public transportation system during the Cold War era (1945-1989). During this period, Berlin was divided, with separate transportation authorities operating in East and West Berlin. This division is reflected in our data processing approach, where we handle each side separately for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEAR = 1967\n",
    "SIDE = \"east\"  # or \"east\"\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Initialize loader\n",
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:24:19,562 - INFO - Loaded raw data: 74 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of loaded data:\n",
      "  line_name  type                                              stops\n",
      "0         3  tram  Revalerstr. - Warschauerstr. - Holteistr. Ecke...\n",
      "1         4  tram  Markusstr. - Ostbahnhof - Mühlenstr. - Revaler...\n",
      "2        13  tram  Hönower Wiesenweg Ecke Blockdammweg - Kraftwer...\n",
      "3        22  tram  Rosenthal, Hauptstr. - Nordendstr. - Kurt-Fisc...\n",
      "4        46  tram  Nordend - Ossietzkyplatz - Kurt-Fischer-Platz ...\n"
     ]
    }
   ],
   "source": [
    "# Load raw transcribed data\n",
    "raw_data_path = DATA_DIR / 'raw' / f'{YEAR}_{SIDE}.csv'\n",
    "raw_df = loader.load_raw_data(str(raw_data_path))\n",
    "logger.info(f\"Loaded raw data: {len(raw_df)} lines\")\n",
    "\n",
    "# Display sample of loaded data to verify\n",
    "print(\"\\nSample of loaded data:\")\n",
    "print(raw_df[['line_name', 'type', 'stops']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Station Reference Data\n",
    "\n",
    "To ensure consistency across years and facilitate geolocation, we maintain a reference dataset of known stations. This dataset:\n",
    "\n",
    "1. Serves as a lookup table for station coordinates\n",
    "2. Helps standardize station names across different time periods\n",
    "3. Provides unique identifiers for stations that persist across snapshots\n",
    "4. Records the lines that serve each station through time\n",
    "\n",
    "As we process new data, this reference dataset will be expanded with newly identified stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Processing\n",
    "\n",
    "The TransportDataProcessor class transforms our raw data into structured tables:\n",
    "\n",
    "1. **Lines Table**: Contains information about each transportation line\n",
    "   - Unique identifiers\n",
    "   - Type (U-Bahn, S-Bahn, tram, bus)\n",
    "   - Terminal stations\n",
    "   - Service frequency\n",
    "   - Journey time and distance\n",
    "\n",
    "2. **Stops Table**: Contains information about each station\n",
    "   - Unique identifiers\n",
    "   - Station names\n",
    "   - Transportation type\n",
    "   - Placeholder for geographic coordinates\n",
    "\n",
    "This structured format facilitates network analysis and visualization in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:24:19,583 - INFO - Using provided DataFrame\n",
      "2025-04-28 14:24:19,613 - INFO - Created tables: lines (74 rows), stops (745 rows), \n",
      "2025-04-28 14:24:19,617 - INFO - Initial processing complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lines table shape: (74, 9)\n",
      "Sample of lines:\n",
      "  line_id  year line_name  type  \\\n",
      "0   19671  1967         3  tram   \n",
      "1   19672  1967         4  tram   \n",
      "\n",
      "                                         start_stop  length (time)  \\\n",
      "0                        Revalerstr.<> Björnsonstr.           42.0   \n",
      "1  Markusstr.<> Eberswalderstr. Ecke Oderbergerstr.           36.0   \n",
      "\n",
      "   length (km) east_west  frequency (7:30)  \n",
      "0          NaN      east              15.0  \n",
      "1          NaN      east               8.0  \n",
      "\n",
      "stops table shape: (745, 6)\n",
      "Sample of stops:\n",
      "        stop_name  type line_name     stop_id location identifier\n",
      "0     Revalerstr.  tram         3  19670_east                    \n",
      "1  Warschauerstr.  tram         3  19671_east                    \n"
     ]
    }
   ],
   "source": [
    "# Process cleaned raw data\n",
    "processor = TransportDataProcessor(YEAR, SIDE)\n",
    "\n",
    "try:\n",
    "    # Pass only the raw DataFrame - existing_stations_df is not used by the processor\n",
    "    results = processor.process_raw_data(raw_df) # <-- MODIFIED HERE\n",
    "    logger.info(\"Initial processing complete\")\n",
    "\n",
    "    # Display processing results\n",
    "    for name, df in results.items():\n",
    "        print(f\"\\n{name} table shape: {df.shape}\")\n",
    "        print(f\"Sample of {name}:\")\n",
    "        print(df.head(2))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in initial processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station Matching\n",
    "\n",
    "This step attempts to match stations in our current dataset with those in our reference database. This process:\n",
    "\n",
    "1. Compares station names and types to find potential matches\n",
    "2. Assigns geographic coordinates from matched stations\n",
    "3. Identifies stations that require manual geolocation\n",
    "4. Logs matching statistics for quality control\n",
    "\n",
    "Stations that cannot be automatically matched will be processed manually using OpenRefine in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:24:19,939 - WARNING - No previous year found with data for side 'east' before 1967.\n",
      "2025-04-28 14:24:19,942 - WARNING - Could not fetch historical data. Cannot perform matching.\n",
      "2025-04-28 14:24:19,946 - INFO - Neo4j connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the new DataFrame-based station matcher\n",
    "df_matcher = DataFrameStationMatcher(\n",
    "    uri=\"bolt://100.82.176.18:7687\", # Or your Neo4j URI\n",
    "    username=\"neo4j\",\n",
    "    password=\"BerlinTransport2024\"\n",
    ")\n",
    "\n",
    "# Process stops table with location matching using the new matcher\n",
    "# Pass the current_year and side to fetch the correct historical data\n",
    "matched_stops = df_matcher.add_location_data(results['stops'], YEAR, SIDE, score_cutoff=85) \n",
    "\n",
    "# Close the Neo4j connection when done with the matcher instance\n",
    "df_matcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching Statistics:\n",
      "Total stations: 745\n",
      "Matched: 0 (0.0%)\n",
      "Unmatched: 745 (100.0%)\n",
      "\n",
      "Sample of matched stations:\n",
      "No stations were matched.\n",
      "\n",
      "Sample of unmatched stations:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "stop_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "line_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stop_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "latitude",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "longitude",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "match_score",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "matched_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "matched_stop_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "matched_historical_lines",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "14769d92-0087-495a-9e0a-e526022b158d",
       "rows": [
        [
         "0",
         "Revalerstr.",
         "tram",
         "3",
         "19670_east",
         "",
         "",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Warschauerstr.",
         "tram",
         "3",
         "19671_east",
         "",
         "",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "Holteistr. Ecke Boxhagenerstr.",
         "tram",
         "3",
         "19672_east",
         "",
         "",
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>type</th>\n",
       "      <th>line_name</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>location</th>\n",
       "      <th>identifier</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>match_score</th>\n",
       "      <th>matched_name</th>\n",
       "      <th>matched_stop_id</th>\n",
       "      <th>matched_historical_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revalerstr.</td>\n",
       "      <td>tram</td>\n",
       "      <td>3</td>\n",
       "      <td>19670_east</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Warschauerstr.</td>\n",
       "      <td>tram</td>\n",
       "      <td>3</td>\n",
       "      <td>19671_east</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Holteistr. Ecke Boxhagenerstr.</td>\n",
       "      <td>tram</td>\n",
       "      <td>3</td>\n",
       "      <td>19672_east</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        stop_name  type line_name     stop_id location  \\\n",
       "0                     Revalerstr.  tram         3  19670_east            \n",
       "1                  Warschauerstr.  tram         3  19671_east            \n",
       "2  Holteistr. Ecke Boxhagenerstr.  tram         3  19672_east            \n",
       "\n",
       "  identifier latitude longitude match_score matched_name matched_stop_id  \\\n",
       "0                None      None        None         None            None   \n",
       "1                None      None        None         None            None   \n",
       "2                None      None        None         None            None   \n",
       "\n",
       "  matched_historical_lines  \n",
       "0                     None  \n",
       "1                     None  \n",
       "2                     None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analysis of matching results\n",
    "total_stops = len(matched_stops)\n",
    "\n",
    "# Check for matches based on the presence of latitude data,\n",
    "# which is only added by the matcher on success.\n",
    "matched = matched_stops['latitude'].notna().sum()\n",
    "unmatched = total_stops - matched\n",
    "\n",
    "print(\"\\nMatching Statistics:\")\n",
    "print(f\"Total stations: {total_stops}\")\n",
    "print(f\"Matched: {matched} ({matched/total_stops*100:.1f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched/total_stops*100:.1f}%)\")\n",
    "\n",
    "# Display sample of matched stations (using latitude check)\n",
    "print(\"\\nSample of matched stations:\")\n",
    "# Display only if there are matched stations to avoid errors/empty output\n",
    "if matched > 0:\n",
    "    display(matched_stops[matched_stops['latitude'].notna()].head(min(3, matched)))\n",
    "else:\n",
    "    print(\"No stations were matched.\")\n",
    "\n",
    "# Display sample of unmatched stations (using latitude check)\n",
    "print(\"\\nSample of unmatched stations:\")\n",
    "# Display only if there are unmatched stations\n",
    "if unmatched > 0:\n",
    "    display(matched_stops[matched_stops['latitude'].isna()].head(min(3, unmatched)))\n",
    "else:\n",
    "    print(\"All stations were matched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_stops = pd.read_csv('../legacy_data/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_stops = matched_stops[matched_stops['latitude'].isna()]\n",
    "\n",
    "# Perform matching based on stop_name, type, and line_name in in_lines\n",
    "for index, row in unmatched_stops.iterrows():\n",
    "    # Filter legacy_stops for matching stop_name and type\n",
    "    potential_matches = legacy_stops[\n",
    "        (legacy_stops['stop_name'] == row['stop_name']) &\n",
    "        (legacy_stops['type'] == row['type'])\n",
    "    ]\n",
    "    \n",
    "    # Check if line_name from unmatched_stops is in in_lines of legacy_stops\n",
    "    closest_match = None\n",
    "    closest_year_diff = float('inf')\n",
    "    \n",
    "    for _, match in potential_matches.iterrows():\n",
    "        # Extract the year from the stop_id of the match\n",
    "        match_year = int(str(match['stop_id'])[:4])  # Assuming the year is the first 4 digits of stop_id\n",
    "        year_diff = abs(YEAR - match_year)\n",
    "        \n",
    "        if year_diff < closest_year_diff:\n",
    "            closest_year_diff = year_diff\n",
    "            closest_match = match\n",
    "    \n",
    "    if closest_match is not None:\n",
    "        # Update location and identifier in unmatched_stops\n",
    "        unmatched_stops.at[index, 'location'] = closest_match['location']\n",
    "        unmatched_stops.at[index, 'location_from'] = closest_match['stop_id']\n",
    "        unmatched_stops.at[index, 'identifier'] = closest_match['identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with non-null location from unmatched_stops\n",
    "newly_matched = unmatched_stops[unmatched_stops['location'].notna() & (unmatched_stops['location'] != \"\")].copy()\n",
    "\n",
    "# Update matched_stops with values from newly_matched based on stop_id\n",
    "for index, row in newly_matched.iterrows():\n",
    "    matched_stops.loc[matched_stops['stop_id'] == row['stop_id'], ['location', 'identifier', 'location_from']] = row[['location', 'identifier', 'location_from']].values\n",
    "\n",
    "# Update unmatched_stops to only include rows with null location\n",
    "unmatched_stops = unmatched_stops[unmatched_stops['location'].isna() | (unmatched_stops['location'] == \"\")].copy()\n",
    "\n",
    "matched_stops = matched_stops[matched_stops['location'].notna() & (matched_stops['location'] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 577 (77.4%)\n",
      "Unmatched: 168 (22.6%)\n"
     ]
    }
   ],
   "source": [
    "matched = len(matched_stops)\n",
    "unmatched = len(unmatched_stops)\n",
    "\n",
    "print(f\"Matched: {matched} ({matched/total_stops*100:.1f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched/total_stops*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 577 total stops to ..\\data\\interim\\stops_matched_initial\\stops_1967_east.csv\n",
      "Exported 168 unmatched stops for manual processing to ..\\data\\interim\\stops_for_openrefine\\unmatched_stops_1967_east.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save results ---\n",
    "matched_dir = Path('../data/interim/stops_matched_initial')\n",
    "matched_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all stops (both matched and unmatched)\n",
    "matched_path = matched_dir / f'stops_{YEAR}_{SIDE}.csv'\n",
    "matched_stops.to_csv(matched_path, index=False)\n",
    "print(f\"\\nSaved {len(matched_stops)} total stops to {matched_path}\")\n",
    "\n",
    "# Save unmatched stops separately for OpenRefine based on missing latitude\n",
    "openrefine_dir = Path('../data/interim/stops_for_openrefine')\n",
    "openrefine_dir.mkdir(parents=True, exist_ok=True)\n",
    "openrefine_path = openrefine_dir / f'unmatched_stops_{YEAR}_{SIDE}.csv'\n",
    "unmatched_stops.to_csv(openrefine_path, index=False)\n",
    "print(f\"Exported {len(unmatched_stops)} unmatched stops for manual processing to {openrefine_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Export\n",
    "\n",
    "As a final step, we validate the matched stations and export the results:\n",
    "\n",
    "1. The complete dataset is saved for the next processing stage\n",
    "2. Unmatched stations are exported separately for manual geolocation\n",
    "3. Matching statistics are logged for quality assurance\n",
    "\n",
    "The manual geolocation process will be performed using OpenRefine, which provides tools for interactive data cleaning and enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After this initial processing, the workflow continues with:\n",
    "\n",
    "1. **Manual Geolocation**: Using OpenRefine to add coordinates to unmatched stations\n",
    "2. **Geolocation Verification**: Validating coordinates and splitting composite stations\n",
    "3. **Data Enrichment**: Adding administrative and contextual information\n",
    "4. **Network Construction**: Building a graph representation of the transportation system\n",
    "5. **Analysis**: Investigating network properties and evolution over time\n",
    "\n",
    "The next notebook in the sequence is `01_geolocation_verification_splitting.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
