{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_process_data.ipynb\n",
    "\n",
    "\"\"\"\n",
    "This notebook processes raw Berlin transport data from Fahrplanbücher into structured formats.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Import processing modules\n",
    "from utils.data_loader import DataLoader, format_line_list\n",
    "from processor import TransportDataProcessor\n",
    "from db_station_matcher import Neo4jStationMatcher\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Transport Data Processing\n",
    "\n",
    "This notebook performs the initial extraction and transformation of Berlin's historical public transportation data from raw sources. It represents the first step in our processing pipeline.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "1. **Data Extraction**: Load and parse raw data from digitized Fahrplanbücher (timetables)\n",
    "2. **Initial Structuring**: Convert raw data into structured tables with consistent formats\n",
    "3. **Station Identification**: Establish unique identifiers for transportation stops\n",
    "4. **Preliminary Geolocation**: Match stations to known geographic coordinates where possible\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process follows these key steps:\n",
    "1. Load raw data from CSV files containing transcribed Fahrplanbuch information\n",
    "2. Process this data into standardized tables (lines and stops)\n",
    "3. Match stations with existing station records to obtain geographic coordinates\n",
    "4. Generate interim data files for subsequent processing stages\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "The data represents Berlin's public transportation system during the Cold War era (1945-1989). During this period, Berlin was divided, with separate transportation authorities operating in East and West Berlin. This division is reflected in our data processing approach, where we handle each side separately for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEAR = 1965\n",
    "SIDE = \"east\"  # or \"east\"\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Initialize loader\n",
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw transcribed data\n",
    "raw_data_path = DATA_DIR / 'raw' / f'{YEAR}_{SIDE}.csv'\n",
    "raw_df = loader.load_raw_data(str(raw_data_path))\n",
    "logger.info(f\"Loaded raw data: {len(raw_df)} lines\")\n",
    "\n",
    "# Display sample of loaded data to verify\n",
    "print(\"\\nSample of loaded data:\")\n",
    "print(raw_df[['line_name', 'type', 'stops']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Station Reference Data\n",
    "\n",
    "To ensure consistency across years and facilitate geolocation, we maintain a reference dataset of known stations. This dataset:\n",
    "\n",
    "1. Serves as a lookup table for station coordinates\n",
    "2. Helps standardize station names across different time periods\n",
    "3. Provides unique identifiers for stations that persist across snapshots\n",
    "4. Records the lines that serve each station through time\n",
    "\n",
    "As we process new data, this reference dataset will be expanded with newly identified stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neo4j station matcher\n",
    "neo4j_matcher = Neo4jStationMatcher(\n",
    "    uri=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"BerlinTransport2024\"\n",
    ")\n",
    "\n",
    "# Get existing stations from Neo4j\n",
    "existing_stations_df = neo4j_matcher.get_all_stations()\n",
    "logger.info(f\"Loaded existing stations from Neo4j: {len(existing_stations_df)} stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Processing\n",
    "\n",
    "The TransportDataProcessor class transforms our raw data into structured tables:\n",
    "\n",
    "1. **Lines Table**: Contains information about each transportation line\n",
    "   - Unique identifiers\n",
    "   - Type (U-Bahn, S-Bahn, tram, bus)\n",
    "   - Terminal stations\n",
    "   - Service frequency\n",
    "   - Journey time and distance\n",
    "\n",
    "2. **Stops Table**: Contains information about each station\n",
    "   - Unique identifiers\n",
    "   - Station names\n",
    "   - Transportation type\n",
    "   - Placeholder for geographic coordinates\n",
    "\n",
    "This structured format facilitates network analysis and visualization in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process cleaned raw data\n",
    "processor = TransportDataProcessor(YEAR, SIDE)\n",
    "\n",
    "try:\n",
    "    # Pass the DataFrame directly\n",
    "    results = processor.process_raw_data(raw_df, existing_stations_df)\n",
    "    logger.info(\"Initial processing complete\")\n",
    "    \n",
    "    # Display processing results\n",
    "    for name, df in results.items():\n",
    "        print(f\"\\n{name} table shape: {df.shape}\")\n",
    "        print(f\"Sample of {name}:\")\n",
    "        display(df.head(2))  # Using display for better notebook output\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in initial processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station Matching\n",
    "\n",
    "This step attempts to match stations in our current dataset with those in our reference database. This process:\n",
    "\n",
    "1. Compares station names and types to find potential matches\n",
    "2. Assigns geographic coordinates from matched stations\n",
    "3. Identifies stations that require manual geolocation\n",
    "4. Logs matching statistics for quality control\n",
    "\n",
    "Stations that cannot be automatically matched will be processed manually using OpenRefine in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = neo4j_matcher\n",
    "\n",
    "# Process stops table with location matching\n",
    "matched_stops = matcher.add_location_data(results['stops'])\n",
    "\n",
    "# Don't forget to close the Neo4j connection when done\n",
    "matcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of matching results\n",
    "total_stops = len(matched_stops)\n",
    "matched = matched_stops['location'].notna().sum()\n",
    "unmatched = total_stops - matched\n",
    "\n",
    "print(\"\\nMatching Statistics:\")\n",
    "print(f\"Total stations: {total_stops}\")\n",
    "print(f\"Matched: {matched} ({matched/total_stops*100:.1f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched/total_stops*100:.1f}%)\")\n",
    "\n",
    "# Display sample of matched stations\n",
    "print(\"\\nSample of matched stations:\")\n",
    "display(matched_stops[matched_stops['location'].notna()].head(3))\n",
    "\n",
    "print(\"\\nSample of unmatched stations:\")\n",
    "display(matched_stops[matched_stops['location'].isna()].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Export\n",
    "\n",
    "As a final step, we validate the matched stations and export the results:\n",
    "\n",
    "1. The complete dataset is saved for the next processing stage\n",
    "2. Unmatched stations are exported separately for manual geolocation\n",
    "3. Matching statistics are logged for quality assurance\n",
    "\n",
    "The manual geolocation process will be performed using OpenRefine, which provides tools for interactive data cleaning and enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate matches\n",
    "from utils.geolocation import validate_matches\n",
    "validate_matches(matched_stops)\n",
    "\n",
    "# Save results\n",
    "matched_dir = Path('../data/interim/stops_matched_initial')\n",
    "matched_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all stops (both matched and unmatched)\n",
    "matched_path = matched_dir / f'stops_{YEAR}_{SIDE}.csv'\n",
    "matched_stops.to_csv(matched_path, index=False)\n",
    "\n",
    "# Save unmatched stops separately for OpenRefine\n",
    "unmatched_stops = matched_stops[matched_stops['location'].isna()]\n",
    "openrefine_dir = Path('../data/interim/stops_for_openrefine')\n",
    "openrefine_dir.mkdir(parents=True, exist_ok=True)\n",
    "openrefine_path = openrefine_dir / f'unmatched_stops_{YEAR}_{SIDE}.csv'\n",
    "unmatched_stops.to_csv(openrefine_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(matched_stops)} total stops\")\n",
    "print(f\"Exported {len(unmatched_stops)} unmatched stops for manual processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After this initial processing, the workflow continues with:\n",
    "\n",
    "1. **Manual Geolocation**: Using OpenRefine to add coordinates to unmatched stations\n",
    "2. **Geolocation Verification**: Validating coordinates and splitting composite stations\n",
    "3. **Data Enrichment**: Adding administrative and contextual information\n",
    "4. **Network Construction**: Building a graph representation of the transportation system\n",
    "5. **Analysis**: Investigating network properties and evolution over time\n",
    "\n",
    "The next notebook in the sequence is `01_geolocation_verification_splitting.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
