{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_process_data.ipynb\n",
    "\n",
    "\"\"\"\n",
    "This notebook processes raw Berlin transport data from Fahrplanbücher into structured formats.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Import processing modules\n",
    "from utils.data_loader import DataLoader, format_line_list\n",
    "from processor import TransportDataProcessor\n",
    "# --- Import the correct matcher ---\n",
    "from df_station_matcher import DataFrameStationMatcher\n",
    "# from db_station_matcher import Neo4jStationMatcher # <-- Can likely remove this import\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Transport Data Processing\n",
    "\n",
    "This notebook performs the initial extraction and transformation of Berlin's historical public transportation data from raw sources. It represents the first step in our processing pipeline.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "1. **Data Extraction**: Load and parse raw data from digitized Fahrplanbücher (timetables)\n",
    "2. **Initial Structuring**: Convert raw data into structured tables with consistent formats\n",
    "3. **Station Identification**: Establish unique identifiers for transportation stops\n",
    "4. **Preliminary Geolocation**: Match stations to known geographic coordinates where possible\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "The process follows these key steps:\n",
    "1. Load raw data from CSV files containing transcribed Fahrplanbuch information\n",
    "2. Process this data into standardized tables (lines and stops)\n",
    "3. Match stations with existing station records to obtain geographic coordinates\n",
    "4. Generate interim data files for subsequent processing stages\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "The data represents Berlin's public transportation system during the Cold War era (1945-1989). During this period, Berlin was divided, with separate transportation authorities operating in East and West Berlin. This division is reflected in our data processing approach, where we handle each side separately for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEAR = 1967\n",
    "SIDE = \"east\"  # or \"east\"\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Initialize loader\n",
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw transcribed data\n",
    "raw_data_path = DATA_DIR / 'raw' / f'{YEAR}_{SIDE}.csv'\n",
    "raw_df = loader.load_raw_data(str(raw_data_path))\n",
    "logger.info(f\"Loaded raw data: {len(raw_df)} lines\")\n",
    "\n",
    "# Display sample of loaded data to verify\n",
    "print(\"\\nSample of loaded data:\")\n",
    "print(raw_df[['line_name', 'type', 'stops']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Station Reference Data\n",
    "\n",
    "To ensure consistency across years and facilitate geolocation, we maintain a reference dataset of known stations. This dataset:\n",
    "\n",
    "1. Serves as a lookup table for station coordinates\n",
    "2. Helps standardize station names across different time periods\n",
    "3. Provides unique identifiers for stations that persist across snapshots\n",
    "4. Records the lines that serve each station through time\n",
    "\n",
    "As we process new data, this reference dataset will be expanded with newly identified stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Processing\n",
    "\n",
    "The TransportDataProcessor class transforms our raw data into structured tables:\n",
    "\n",
    "1. **Lines Table**: Contains information about each transportation line\n",
    "   - Unique identifiers\n",
    "   - Type (U-Bahn, S-Bahn, tram, bus)\n",
    "   - Terminal stations\n",
    "   - Service frequency\n",
    "   - Journey time and distance\n",
    "\n",
    "2. **Stops Table**: Contains information about each station\n",
    "   - Unique identifiers\n",
    "   - Station names\n",
    "   - Transportation type\n",
    "   - Placeholder for geographic coordinates\n",
    "\n",
    "This structured format facilitates network analysis and visualization in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process cleaned raw data\n",
    "processor = TransportDataProcessor(YEAR, SIDE)\n",
    "\n",
    "try:\n",
    "    # Pass only the raw DataFrame - existing_stations_df is not used by the processor\n",
    "    results = processor.process_raw_data(raw_df) # <-- MODIFIED HERE\n",
    "    logger.info(\"Initial processing complete\")\n",
    "\n",
    "    # Display processing results\n",
    "    for name, df in results.items():\n",
    "        print(f\"\\n{name} table shape: {df.shape}\")\n",
    "        print(f\"Sample of {name}:\")\n",
    "        print(df.head(2))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in initial processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station Matching\n",
    "\n",
    "This step attempts to match stations in our current dataset with those in our reference database. This process:\n",
    "\n",
    "1. Compares station names and types to find potential matches\n",
    "2. Assigns geographic coordinates from matched stations\n",
    "3. Identifies stations that require manual geolocation\n",
    "4. Logs matching statistics for quality control\n",
    "\n",
    "Stations that cannot be automatically matched will be processed manually using OpenRefine in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new DataFrame-based station matcher\n",
    "df_matcher = DataFrameStationMatcher(\n",
    "    uri=\"bolt://100.82.176.18:7687\", # Or your Neo4j URI\n",
    "    username=\"neo4j\",\n",
    "    password=\"BerlinTransport2024\"\n",
    ")\n",
    "\n",
    "# Process stops table with location matching using the new matcher\n",
    "# Pass the current_year and side to fetch the correct historical data\n",
    "matched_stops = df_matcher.add_location_data(results['stops'], YEAR, SIDE, score_cutoff=85) \n",
    "\n",
    "# Close the Neo4j connection when done with the matcher instance\n",
    "df_matcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of matching results\n",
    "total_stops = len(matched_stops)\n",
    "\n",
    "# Check for matches based on the presence of latitude data,\n",
    "# which is only added by the matcher on success.\n",
    "matched = matched_stops['latitude'].notna().sum()\n",
    "unmatched = total_stops - matched\n",
    "\n",
    "print(\"\\nMatching Statistics:\")\n",
    "print(f\"Total stations: {total_stops}\")\n",
    "print(f\"Matched: {matched} ({matched/total_stops*100:.1f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched/total_stops*100:.1f}%)\")\n",
    "\n",
    "# Display sample of matched stations (using latitude check)\n",
    "print(\"\\nSample of matched stations:\")\n",
    "# Display only if there are matched stations to avoid errors/empty output\n",
    "if matched > 0:\n",
    "    display(matched_stops[matched_stops['latitude'].notna()].head(min(3, matched)))\n",
    "else:\n",
    "    print(\"No stations were matched.\")\n",
    "\n",
    "# Display sample of unmatched stations (using latitude check)\n",
    "print(\"\\nSample of unmatched stations:\")\n",
    "# Display only if there are unmatched stations\n",
    "if unmatched > 0:\n",
    "    display(matched_stops[matched_stops['latitude'].isna()].head(min(3, unmatched)))\n",
    "else:\n",
    "    print(\"All stations were matched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_stops = pd.read_csv('../legacy_data/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_stops = matched_stops[matched_stops['latitude'].isna()]\n",
    "\n",
    "# Perform matching based on stop_name, type, and line_name in in_lines\n",
    "for index, row in unmatched_stops.iterrows():\n",
    "    # Filter legacy_stops for matching stop_name and type\n",
    "    potential_matches = legacy_stops[\n",
    "        (legacy_stops['stop_name'] == row['stop_name']) &\n",
    "        (legacy_stops['type'] == row['type'])\n",
    "    ]\n",
    "    \n",
    "    # Check if line_name from unmatched_stops is in in_lines of legacy_stops\n",
    "    closest_match = None\n",
    "    closest_year_diff = float('inf')\n",
    "    \n",
    "    for _, match in potential_matches.iterrows():\n",
    "        # Extract the year from the stop_id of the match\n",
    "        match_year = int(str(match['stop_id'])[:4])  # Assuming the year is the first 4 digits of stop_id\n",
    "        year_diff = abs(YEAR - match_year)\n",
    "        \n",
    "        if year_diff < closest_year_diff:\n",
    "            closest_year_diff = year_diff\n",
    "            closest_match = match\n",
    "    \n",
    "    if closest_match is not None:\n",
    "        # Update location and identifier in unmatched_stops\n",
    "        unmatched_stops.at[index, 'location'] = closest_match['location']\n",
    "        unmatched_stops.at[index, 'location_from'] = closest_match['stop_id']\n",
    "        unmatched_stops.at[index, 'identifier'] = closest_match['identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with non-null location from unmatched_stops\n",
    "newly_matched = unmatched_stops[unmatched_stops['location'].notna() & (unmatched_stops['location'] != \"\")].copy()\n",
    "\n",
    "# Update matched_stops with values from newly_matched based on stop_id\n",
    "for index, row in newly_matched.iterrows():\n",
    "    matched_stops.loc[matched_stops['stop_id'] == row['stop_id'], ['location', 'identifier', 'location_from']] = row[['location', 'identifier', 'location_from']].values\n",
    "\n",
    "# Update unmatched_stops to only include rows with null location\n",
    "unmatched_stops = unmatched_stops[unmatched_stops['location'].isna() | (unmatched_stops['location'] == \"\")].copy()\n",
    "\n",
    "matched_stops = matched_stops[matched_stops['location'].notna() & (matched_stops['location'] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = len(matched_stops)\n",
    "unmatched = len(unmatched_stops)\n",
    "\n",
    "print(f\"Matched: {matched} ({matched/total_stops*100:.1f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched/total_stops*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save results ---\n",
    "matched_dir = Path('../data/interim/stops_matched_initial')\n",
    "matched_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all stops (both matched and unmatched)\n",
    "matched_path = matched_dir / f'stops_{YEAR}_{SIDE}.csv'\n",
    "matched_stops.to_csv(matched_path, index=False)\n",
    "print(f\"\\nSaved {len(matched_stops)} total stops to {matched_path}\")\n",
    "\n",
    "# Save unmatched stops separately for OpenRefine based on missing latitude\n",
    "openrefine_dir = Path('../data/interim/stops_for_openrefine')\n",
    "openrefine_dir.mkdir(parents=True, exist_ok=True)\n",
    "openrefine_path = openrefine_dir / f'unmatched_stops_{YEAR}_{SIDE}.csv'\n",
    "unmatched_stops.to_csv(openrefine_path, index=False)\n",
    "print(f\"Exported {len(unmatched_stops)} unmatched stops for manual processing to {openrefine_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Export\n",
    "\n",
    "As a final step, we validate the matched stations and export the results:\n",
    "\n",
    "1. The complete dataset is saved for the next processing stage\n",
    "2. Unmatched stations are exported separately for manual geolocation\n",
    "3. Matching statistics are logged for quality assurance\n",
    "\n",
    "The manual geolocation process will be performed using OpenRefine, which provides tools for interactive data cleaning and enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After this initial processing, the workflow continues with:\n",
    "\n",
    "1. **Manual Geolocation**: Using OpenRefine to add coordinates to unmatched stations\n",
    "2. **Geolocation Verification**: Validating coordinates and splitting composite stations\n",
    "3. **Data Enrichment**: Adding administrative and contextual information\n",
    "4. **Network Construction**: Building a graph representation of the transportation system\n",
    "5. **Analysis**: Investigating network properties and evolution over time\n",
    "\n",
    "The next notebook in the sequence is `01_geolocation_verification_splitting.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
